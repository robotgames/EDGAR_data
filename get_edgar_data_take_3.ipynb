{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the company tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we can obtain the company tickers for the publicly traded funds.  SEC maintains a list in a `json` file.  Tickers can be useful for Google searches; they are unique, and are much more common than the SEC CIK identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'User-Agent':'robot.games@gmail.com'}\n",
    "temp = requests.get(\"https://www.sec.gov/files/company_tickers.json\",headers=headers).json()\n",
    "company_tickers = pd.DataFrame(temp.values()).rename({'cik_str':'cik'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(company_tickers.shape)\n",
    "company_tickers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Filer's SEC Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.sec.gov/open/datasets-investment_company for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "info = pd.read_csv(\"data/investment_company_series_class_2020.csv\",dtype = {\"CIK Number\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to grab only certain types of entity, as classified by the `Entity Org Type` column.  How are the values there distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['Entity Org Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(info['Entity Org Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want type 30.  Let's get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = info[info[\"Entity Org Type\"]==30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get and store the allowed CIK numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_filers = (\n",
    "    info[[\"CIK Number\",'Entity Name','Series ID','Series Name']]\n",
    "    .rename(columns = {'CIK Number':'cik','Entity Name':'name','Series ID':'series_id','Series Name':'series_name'})\n",
    ")\n",
    "allowed_filers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join data to this later and remove NA rows to filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Edgar API information\n",
    "\n",
    "The API information allows us to search for the keys that let us open the NPORT documents.  So...\n",
    "1. Get API information.\n",
    "2. Extract two numbers that uniquely define the online location of the NPORT form.\n",
    "3. Go get that NPORT form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://www.sec.gov/Archives/edgar/full-index/2020/QTR1/form.zip'\n",
    "download_directory = 'C:\\\\Users\\\\robot\\\\Downloads\\\\'\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(20)\n",
    "# driver.get(url)  # selenium is hanging on this.  Want to fgure this out but have manually downloaded the needed file and stored locally.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the zipfile into the data directory of the project folder.  Then extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data/form.zip') as zip_object:\n",
    "    zip_object.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the index file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('data/form.idx','r') as f:\n",
    "    temp = pd.DataFrame({'line':f.readlines()})\n",
    "temp = temp[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the first line looks like.  Essentially we need to grab the two numbers in the file name, reformat them and that lets us build the name of the file online.  But we only want the information from the NPORT filings; the first line here is from a 1-A filing and so we will not use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(temp.line)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab the URLs for the NPORT forms wherever mentioned in the index file.  There is some `regex` (regular expressions) magic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the NPORT filings\n",
    "nport_lines = temp[ temp.line.str.find('NPORT-')>=0 ]\n",
    "# get the needed data\n",
    "nport_split = nport_lines.line.str.replace('[ \\t\\n]{2,}',' ',regex=True).str.split(' ')\n",
    "# reformat to part of a URL\n",
    "nport_url = [nport_split.iloc[z][-2] for z in range(nport_split.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nport_url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate and download each NPORT form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I performed these operations once and stored the files locally.  No need to repeat, but the code can be easily adapted to another fiancial quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stem = 'https://www.sec.gov/Archives/'\n",
    "nport_txt_doc_url = [url_stem + x for x in nport_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nport_txt_doc_url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of the xml files and store them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "N = len(nport_txt_doc_url)\n",
    "s = requests.Session()\n",
    "for i in range(N):\n",
    "    try:\n",
    "        #print('Working on file '+str(i)+'/'+str(N))\n",
    "        headers = {'User-Agent':'robot.games@gmail.com'}\n",
    "        temp = s.get(nport_txt_doc_url[i],headers=headers,timeout=20)\n",
    "    except:\n",
    "        print('Timeout on item '+str(i))\n",
    "    time.sleep(0.1)\n",
    "    with open('data/xml/'+str(i)+'.txt','wb') as f:\n",
    "        f.write(temp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the XML Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse each xml document (each NPORT form) one by one.  Gather the results into two data frames and store them on disk as csv files.\n",
    "\n",
    "Total execution time: ~ 7 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the initial data buckets.  Dictionaries are easy to work with here and are easy to convert to data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer = {\n",
    "    'name':[],\n",
    "    'cik':[],\n",
    "    'series':[],\n",
    "    'series_id':[],\n",
    "    'total_assets':[],\n",
    "    'total_liabilities':[],\n",
    "    'cash':[],\n",
    "    'file_name':[]\n",
    "}\n",
    "\n",
    "investment = {\n",
    "    'cik':[],\n",
    "    'series_id':[],\n",
    "    'investment_name':[],\n",
    "    'isin':[],\n",
    "    'value_USD':[],\n",
    "    'percentage_investment':[],\n",
    "    'asset_category':[],\n",
    "    'issuer_category':[],\n",
    "    'fair_value_level':[],\n",
    "    'file_name':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a few helper functions to perform the different steps in the extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_filer_information(soup):\n",
    "    regname = soup.select('regName')\n",
    "    if len(regname)>0:\n",
    "        filer_name = regname[0].text.replace(',','')\n",
    "    else:\n",
    "        print('NPORT skipped')\n",
    "        return\n",
    "    cik_value = soup.select('regCik')[0].text\n",
    "    series = soup.select('seriesName')[0].text\n",
    "    series_id = soup.select('seriesId')\n",
    "    if len(series_id)>0:\n",
    "        series_id = series_id[0].text\n",
    "    else:\n",
    "        series_id = None\n",
    "    assets = float(soup.select('totAssets')[0].text)\n",
    "    liabilities = float(soup.select('totLiabs')[0].text)\n",
    "    try:\n",
    "        cash = float(soup.select('cshNotRptdInCorD')[0].text)\n",
    "    except:\n",
    "        cash = 0.0\n",
    "    return (filer_name,cik_value,series,series_id,assets,liabilities,cash)\n",
    "\n",
    "def get_investment_information(node):\n",
    "    try:\n",
    "        isnode = node\n",
    "        while (isnode.name != 'identifiers') and not (isnode is None):\n",
    "            isnode = isnode.next_sibling\n",
    "        if not(isnode is None):\n",
    "            isnode = next(isnode.children,None)\n",
    "            while (isnode.name != 'isin') and not (isnode is None):\n",
    "                isnode = isnode.next_sibling\n",
    "        vnode = node\n",
    "        while (vnode.name != 'valusd') and not(vnode is None):\n",
    "            vnode = vnode.next_sibling\n",
    "        if not(vnode is None):\n",
    "            pnode = vnode\n",
    "        else:\n",
    "            pnode = node\n",
    "        while (pnode.name != 'pctval') and not(pnode is None):\n",
    "            pnode = pnode.next_sibling\n",
    "        if not(pnode is None):\n",
    "            anode = pnode\n",
    "        else:\n",
    "            anode = node\n",
    "        while (anode.name != 'assetcat') and not(anode is None):\n",
    "            anode = anode.next_sibling\n",
    "        if not(anode is None):\n",
    "            inode = anode\n",
    "        else:\n",
    "            inode = node\n",
    "        while (inode.name != 'issuercat') and not(inode is None):\n",
    "            inode = inode.next_sibling\n",
    "        if not(inode is None):\n",
    "            fnode = inode\n",
    "        else:\n",
    "            fnode = node\n",
    "        while (fnode.name != 'fairvallevel') and not(fnode is None):\n",
    "            fnode = fnode.next_sibling\n",
    "        isin = isnode.get('value')\n",
    "        value_USD = float(vnode.text)\n",
    "        percent = float(pnode.text)\n",
    "        asset_type = anode.text\n",
    "        issuer_type = inode.text    \n",
    "        fair_value_level = fnode.text\n",
    "    except:\n",
    "        return\n",
    "    return (isin,value_USD, percent,asset_type,issuer_type,fair_value_level)\n",
    "\n",
    "def parse_xbrl_file(handle):\n",
    "    # Throw everything into beautifulsoup\n",
    "    soup = BeautifulSoup(handle,'lxml')\n",
    "    filer_information = get_filer_information(soup)\n",
    "    # This is tricky because there are some missing values in some files\n",
    "    investment_names = soup.select('name')\n",
    "    print('There are',len(investment_names),'investments to parse')\n",
    "    investment_information = {}\n",
    "    for node in investment_names:\n",
    "         investment_information[node.text] = get_investment_information(node)\n",
    "    return (filer_information,investment_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the operation.  This could possibly be ported to parallel processing and cut the time down by a far bit.  However, I'm only performing these operations once or twice for this research project; if we were performing this operation four times a year I would implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['data/xml/'+z for z in os.listdir('data/xml')]\n",
    "start = 9000\n",
    "end = len(file_names)\n",
    "file_names = file_names[start:end]\n",
    "for i,file_name in enumerate(file_names):\n",
    "        print('Working on',file_name,'which is',i,'/',end-start)\n",
    "        with open(file_name,'r') as handle:\n",
    "                #  Add to the filer database\n",
    "                filer_information,investment_information = parse_xbrl_file(handle)\n",
    "                try:\n",
    "                        filer_name,cik_value,series,series_id,assets,liabilities,cash = filer_information\n",
    "                        filer['name'].append(filer_name)\n",
    "                        filer['cik'].append(cik_value)\n",
    "                        filer['series'].append(series)\n",
    "                        filer['series_id'].append(series_id)\n",
    "                        filer['total_assets'].append(assets)\n",
    "                        filer['total_liabilities'].append(liabilities)\n",
    "                        filer['cash'].append(cash)\n",
    "                        filer['file_name'].append(file_name)\n",
    "                except:\n",
    "                        print('skipping one filer')\n",
    "                for i_name in list(investment_information.keys()):\n",
    "                        print(i_name)\n",
    "                        if not(investment_information[i_name] is None):\n",
    "                                isin,value_USD,percent,asset_type,issuer_type,fair_value_level = investment_information[i_name]\n",
    "                                investment['isin'].append(isin)\n",
    "                                investment['cik'].append(cik_value)\n",
    "                                investment['series_id'].append(series_id)\n",
    "                                investment['investment_name'].append(i_name)\n",
    "                                investment['value_USD'].append(value_USD)\n",
    "                                investment['percentage_investment'].append(percent)\n",
    "                                investment['asset_category'].append(asset_type)\n",
    "                                investment['issuer_category'].append(issuer_type)\n",
    "                                investment['fair_value_level'].append(fair_value_level)\n",
    "                                investment['file_name'].append(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer = pd.DataFrame(filer)\n",
    "investment = pd.DataFrame(investment)\n",
    "filer.replace(',','',regex=True,inplace=True)\n",
    "investment.replace(',','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the initial files to local storage.\n",
    "\n",
    "IF WRITING NEW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer.to_csv('filer_raw.csv',index=False)\n",
    "investment.to_csv('investment_raw.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF APPENDING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filer_disk = pd.read_csv('filer_raw.csv',dtype={'cik':str})\n",
    "investment_disk = pd.read_csv('investment_raw.csv',dtype={'cik':str})\n",
    "filer2 = pd.concat([filer_disk,filer],ignore_index=True)\n",
    "investment2 = pd.concat([investment_disk,investment],ignore_index=True)\n",
    "filer2.to_csv('filer_raw.csv',index=False)\n",
    "investment2.to_csv('investment_raw.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore, filter, and validate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the raw csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filer = pd.read_csv('filer_raw.csv',dtype={'cik':str})\n",
    "investment = pd.read_csv('investment_raw.csv',dtype={'cik':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filer` data contains data about the entity filing the NPORT form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter that for the allowed cik and series values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer = (\n",
    "    allowed_filers\n",
    "    [['cik','series_id']]\n",
    "    .merge(filer,how='right',on=['cik','series_id'])\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    ")\n",
    "investment = (\n",
    "    allowed_filers\n",
    "    [['cik','series_id']]\n",
    "    .merge(investment,how='right',on=['cik','series_id'])\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_summary = (\n",
    "    investment\n",
    "    [['cik','series_id','value_USD','percentage_investment']]\n",
    "    .groupby(['cik','series_id'])\n",
    "    .sum()\n",
    "    .rename(columns={'value_USD':'total_value_USD','percentage_investment':'total_percentage_invested'})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment = (\n",
    "    investment\n",
    "    .merge(investment_summary,how='left',on=['cik','series_id'])\n",
    ")\n",
    "investment['fraction_of_value'] = investment.value_USD / investment.total_value_USD\n",
    "investment['fraction_of_stated_percentage'] = investment.percentage_investment / investment.total_percentage_invested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer.to_csv('filer_raw2.csv',index=False)\n",
    "investment.to_csv('investment_raw2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the file for upload to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def write_chunk_csv(df,n,file_stem='out'):\n",
    "    N = round(df.shape[0]/n)\n",
    "    i=0\n",
    "    while i < n-1:\n",
    "        temp = df.iloc[(i*N):((i+1)*N),:]\n",
    "        temp.to_csv(file_stem+'_'+str(i)+'.csv',index=False)\n",
    "        i+=1\n",
    "    temp = df.iloc[((n-1)*N):,:]\n",
    "    temp.to_csv(file_stem+'_'+str(n-1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_chunk_csv(investment,20,file_stem='investment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
